# 可微分物理导论

## 目录
- 可微分算子
- 雅可比矩阵
- 通过可微分物理算子学习
- 一个实际例子
- 隐式梯度计算
- 可微分物理方法总结

作为朝着深度学习方法和物理模拟更紧密、更通用结合迈出的下一步，我们将目标定为将**可微分数值模拟**融入学习过程。在下文中，我们将把这些“物理系统的可微分数值模拟”简称为“可微分物理”（Differentiable Physics, DP）。

这些方法的中心目标是使用现有的数值求解器来增强和改进人工智能系统。这需要为它们配备计算关于其输入的梯度的功能。一旦对模拟的所有算子实现了这一点，我们就可以利用深度学习框架具有反向传播功能的自动微分（autodiff），让梯度信息从模拟器流入神经网络（NN），反之亦然。这具有许多优点，例如改进的学习反馈和泛化能力，我们将在下面概述。

与上一章中基于物理信息的损失函数相比，它还能够处理更复杂的解流形，而不是单一的逆向问题。例如，与上一章使用深度学习解决单一逆向问题不同，可微分物理可用于训练神经网络，使其学会高效解决更大类别的逆向问题。

> **图 15** 使用可微分物理进行训练意味着一系列可微分算子以梯度的形式提供方向来指导学习过程。
>
> ![image-20250831000149421](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250831000149421.png)

## 可微分算子

使用 DP，我们建立在现有的数值求解器之上。即，该方法强烈依赖于更广泛的计算方法领域为世界上各种物理效应开发的算法。首先，我们需要一个连续的公式作为我们想要模拟的物理效应的模型——如果缺少这个，我们就麻烦了。但幸运的是，我们可以利用现有的模型方程集合和离散化连续模型的既定方法。

假设我们有一个感兴趣的物理量 $u(\mathbf{x},t): \mathbb{R}^d \times \mathbb{R}^+ \rightarrow \mathbb{R}^d$ 的连续公式，其模型参数为 $\nu$（例如，扩散、粘度或电导率常数）。$u$ 的分量将由数字下标表示，即 $u = (u_1, u_2,…, u_d)^T$。通常，我们对此类系统的时间演化感兴趣。离散化产生一个公式 $P(u, \nu)$，我们将其重新排列以计算时间步长 $\Delta t$ 后的未来状态。$t + \Delta t$ 时刻的状态通过一系列操作 $P_1, P_2, …, P_m$ 计算，使得 $u(t + \Delta t) = P_m \circ … P_2 \circ P_1(u(t), \nu)$，其中 $\circ$ 表示函数组合，即 $f(g(x)) = f \circ g(x)$。

为了将求解器集成到深度学习过程中，我们需要确保每个算子 $P_i$ 都提供关于其输入的梯度，即在上面的例子中需要 $\partial P_i / \partial u$。

请注意，我们通常不需要 $P(u, \nu)$ 所有参数的导数，例如，我们在下面省略了 $\nu$，假设这是一个给定的模型参数，神经网络不应与之交互。自然，它可以在我们感兴趣的解流形内变化，但不会是神经网络表示的输出。如果是这种情况，我们可以在求解器中省略提供 $\partial P_i / \partial \nu$。然而，下面的学习过程自然可以转移到将 $\nu$ 作为一个自由度包含在内。

(这个过程可以理解为：我们希望通过神经网络来预测一个物理量 $u(\mathbf{x}, t)$ 在未来的状态，类似于解决一个物理方程。为了使神经网络能够进行物理模拟，我们需要确保神经网络不仅能够学习数据的关系，还能遵循物理方程的规律（通过计算梯度和物理损失函数来确保）。在训练过程中，神经网络会基于给定的参数（如 $\nu$）和初始条件来预测物理量的演化。

通常，模型参数 $\nu$ 是已知的，不需要优化，但我们可以将它作为自由度来优化。如果我们将 $\nu$ 作为网络的一个自由度，网络将不仅优化物理量 $u$，还会根据学习到的知识调整 $\nu$，这将帮助网络更好地预测物理系统的行为。)

## 雅可比矩阵

由于 $u$ 通常是向量值函数，$\partial P_i / \partial u$ 表示一个雅可比矩阵（Jacobian matrix），而不是单个值：

$$
J = \frac{\partial P_i}{\partial u} =
\begin{bmatrix}
\partial P_{i,1} / \partial u_1 & \cdots & \partial P_{i,1} / \partial u_d \\
\vdots & \ddots & \vdots \\
\partial P_{i,d} / \partial u_1 & \cdots & \partial P_{i,d} / \partial u_d
\end{bmatrix}
$$

其中，如上所述，$d$ 表示 $u$ 中的分量数。由于 $P$ 将一个 $u$ 值映射到另一个值，这里的雅可比矩阵是方阵。当然，对于一般的模型方程，这不一定是情况，但非方阵的雅可比矩阵不会给可微分模拟带来任何问题。

在实践中，我们依赖于所有现代深度学习框架提供的**反向模式微分**（reverse mode differentiation），并专注于计算雅可比矩阵转置与向量 $\mathbf{a}$ 的矩阵向量积，即表达式：$(\frac{\partial P_i}{\partial u})^T \mathbf{a}$。如果我们需要构建和存储训练过程中遇到的所有完整雅可比矩阵，这将导致巨大的内存开销并不必要地减慢训练速度。相反，对于反向传播，我们可以提供更快的操作来计算与雅可比转置的乘积，因为我们总是在链的末端有一个标量损失函数。

给定上面的公式，我们需要通过链式法则在某当前状态 $u_n$ 下解析函数组合链 $P_i$ 的导数。例如，对于其中两个：

$$
\frac{\partial (P_1 \circ P_2)}{\partial u} \Big|_{u_n} = \frac{\partial P_1}{\partial P_2} \Big|_{P_2(u_n)} \frac{\partial P_2}{\partial u} \Big|_{u_n}
$$

这仅仅是“经典”链式法则 $f(g(x))' = f'(g(x))g'(x)$ 的向量值版本，并且直接扩展到更多组合函数的情况，即 $i > 2$。

这里，$P_1$ 和 $P_2$ 的导数仍然是雅可比矩阵，但是知道在链的“末端”我们有我们的标量损失 $L$（参见概述），最右边的雅可比矩阵将总是一个只有 1 列的矩阵，即一个向量。在反向模式期间，我们从这个向量开始，并依次计算与左边雅可比矩阵的乘法。

关于前向和反向模式微分的细节，请查阅外部材料，例如 Baydin 等人的这篇很好的综述。

( 

**总结**

- **雅可比矩阵**表示输入 $u$ 对输出 $P(u)$ 的变化率，它是一个描述函数变化的矩阵。
- **反向模式微分**让我们能够高效计算梯度，而不需要直接计算和存储所有雅可比矩阵。通过链式法则，反向传播从损失函数开始，逐层计算雅可比矩阵与向量的乘积，最终得到梯度。

)

## 通过 DP 算子学习

因此，一旦我们模拟器的算子支持计算雅可比-向量积，我们就可以将它们集成到深度学习管道中，就像你包含一个常规的全连接层或 ReLU 激活函数一样。

在这一点上，会出现以下（非常合理的）问题：“大多数物理求解器可以分解为一系列向量和矩阵运算。所有最先进的深度学习框架都支持这些运算，那么我们为什么不直接使用这些算子来实现我们的物理求解器呢？”

理论上，这确实是可能的。这里的问题是，TensorFlow 和 PyTorch 中的每个向量和矩阵运算都是单独计算的，并且内部需要存储前向评估的当前状态以进行反向传播（上面的 $g(x)$）。然而，对于一个典型的模拟，我们并不非常关心求解器产生的每一个中间结果。通常，我们更关心重大的更新，例如从 $u(t)$ 到 $u(t + \Delta t)$ 的步骤。

因此，在实践中，将求解过程分解为一系列有意义且 monolithic 的算子是一个非常好的主意。这不仅可以防止计算不必要的中间结果从而节省大量工作，还允许我们选择最佳的数值方法来计算这些算子的更新（和导数）。例如，由于这个过程与伴随方法优化非常相似，我们可以重用在该领域开发的许多技术，或者利用已建立的数值方法。例如，我们可以利用多重网格求解器的运行时来进行矩阵求逆。

这种方法的另一面是，它需要对当前问题和数值方法有一定的理解。而且，一个给定的求解器可能不会开箱即用地提供梯度计算。因此，如果我们想对我们没有很好掌握的模型方程采用深度学习，直接通过 DP 方法进行学习可能不是一个好主意。然而，如果我们真的不理解我们的模型，我们可能应该回过头来再多研究一下……

此外，在实践中，我们应该对导数算子“贪婪”，只提供那些与学习任务相关的导数。例如，如果我们的网络从不产生上面例子中的参数 $\nu$，并且它没有出现在我们的损失公式中，我们在反向传播步骤中永远不会遇到 $\partial / \partial \nu$ 的导数。

下图总结了基于 DP 的学习方法，并说明了通常在单个 PDE 求解中处理的操作序列。由于许多操作在实践中是非线性的，这通常给神经网络带来一个具有挑战性的学习任务：

(

**总结：通过 DP 算子学习的关键点**

- 将物理求解器的算子集成到深度学习中时，我们希望避免计算所有中间结果，而是将模拟过程分解为几个关键的算子。这样，我们不仅节省了内存，还能选择最有效的数值方法来处理每个算子的更新。
- 通过这种方法，我们可以将物理系统的求解与神经网络的训练有效地结合在一起，从而实现基于物理约束的学习。

)

> **图 16** 使用由单个算子 $P_i$ 组成的 PDE 求解器进行 DP 学习。梯度 $\partial L / \partial \theta$ 在影响网络权重 $\theta$ 之前向后穿过所有算子。
>
> ![image-20250831000926034](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250831000926034.png)

## 一个实际例子

作为一个简单的例子，让我们考虑一个被动标量密度 $d(\mathbf{x},t)$ 在速度场 $u$ 中的平流作为物理模型 $P^*$：

$$
\frac{\partial d}{\partial t} + u \cdot \nabla d = 0
$$

与其立即使用这个公式作为残差方程（如物理损失项中的 v2），我们可以用我们最喜欢的网格和离散化方案来离散化它，以获得一个随时间更新系统状态的公式。这是前向求解的标准过程。为了简化问题，我们假设这里 $u$ 只是空间的函数，即随时间不变。我们稍后会 reintroduce $u$ 的时间演化。

让我们将这种重新表述表示为 $P$。它将 $d$ 的一个状态映射到一个演化时间后的新状态，即：

$$
d(t + \Delta t) = P( d(t), u, t + \Delta t)
$$

作为逆向问题和学习任务的一个简单例子，让我们考虑寻找速度场 $u$ 的问题。该速度场应该将给定的初始标量密度状态 $d_0$ 在时间 $t_0$ 转换为一个状态，该状态通过 $P$ 演化到后来的“结束”时间 $t_e$，并具有某种形状或配置 $d_{\text{target}}$。非正式地，我们想找到一个流，通过 PDE 模型将 $d_0$ 变形为目标状态。表达这个目标的最简单方法是通过两个状态之间的 $L_2$ 损失。所以我们想最小化损失函数 $L = |d(t_e) - d_{\text{target}}|^2$。

请注意，如此处所述，这个逆向问题是一个纯粹的优化任务：没有神经网络参与，我们的目标是获得 $u$。我们不想将这个速度应用于其他未见过的测试数据，这在真正的学习任务中通常是惯例。

标记密度 $d$ 的最终状态完全由通过 $P$ 从 $d_0$ 的演化决定，这给出了以下最小化问题：

$$
\arg \min_u |P(d_0, u, t_e) - d_{\text{target}}|^2
$$

我们现在想通过梯度下降（GD）找到这个目标函数的最小值，其中梯度由本章前面描述的可微分物理方法确定。一旦梯度下降工作正常，我们就可以相对容易地切换到更好的优化器或将神经网络引入画面，因此它总是一个好的起点。为了使下面的内容更容易阅读，我们将省略雅可比矩阵的转置。

> **注意**：不幸的是，雅可比矩阵是这样定义的，但我们实际上从来不需要未转置的雅可比矩阵。请记住，在实践中我们处理的是转置的雅可比矩阵 $(\frac{\partial a}{\partial b})^T$，在表示上“缩写”为 $\frac{\partial a}{\partial b}$。

由于离散化的速度场 $u$ 包含我们所有的自由度，我们需要做的就是将速度更新一个量 $\Delta u = \partial L / \partial u$，它可以分解为 $\Delta u = \frac{\partial d}{\partial u} \frac{\partial L}{\partial d}$。

分量 $\frac{\partial L}{\partial d}$ 通常很简单：我们会得到

$$
\frac{\partial L}{\partial d} = \frac{\partial |P(d_0, u, t_e) - d_{\text{target}}|^2}{\partial d} = 2(d(t_e) - d_{\text{target}}).
$$

如果 $d$ 表示为一个向量，例如，对于网格的每个单元有一个条目，$\frac{\partial L}{\partial d}$ 同样将是一个等效大小的列向量。这源于 $L$ 始终是一个标量损失函数的事实，因此雅可比矩阵在 $L$ 维度上的维度将为 1。直观地说，这个向量将简单地包含最终时间 $d(t_e)$ 与目标密度 $d_{\text{target}}$ 相比的差异。

$d$ 本身的演化由我们的离散化物理模型 $P$ 给出，我们互换使用 $P$ 和 $d$。因此，更有趣的分量是雅可比矩阵 $\partial d / \partial u = \partial P / \partial u$，用于计算完整的 $\Delta u = \frac{\partial d}{\partial u} \frac{\partial L}{\partial d}$。幸运的是，我们不需要 $\partial d / \partial u$ 作为一个完整的矩阵，而只需要它乘以 $\frac{\partial L}{\partial d}$。

那么 $\partial P / \partial u$ 的实际雅可比矩阵是什么？为了计算它，我们首先需要完成我们的 PDE 模型 $P$，这样我们就可以得到一个可以求导的表达式。在下一节中，我们将选择一个特定的平流方案和离散化，以便我们可以更具体。

(**总结：逆向问题的优化**

- 通过梯度下降法，我们可以根据标量密度 $d$ 从初始状态 $d_0$ 到目标状态 $d_{\text{target}}$ 的演化，优化速度场 $u$。
- **计算关键**：在优化过程中，我们需要计算损失函数对 $d$ 和 $u$ 的梯度，这两个梯度分别通过标量密度和速度场的演化模型来确定。
- **雅可比矩阵**：雅可比矩阵 $\frac{\partial d}{\partial u}$ 是非常关键的，它表示速度场如何影响标量密度的演化，帮助我们计算速度场的更新量。

**实践中的挑战**

- 在实际应用中，计算这些梯度（尤其是雅可比矩阵）可能需要精心设计和优化数值方法，因为这通常涉及到复杂的物理模型和大量的计算资源。
- 但这种方法提供了一个强大的框架，使我们能够通过物理方程来指导神经网络的学习，确保模型输出不仅仅是数据拟合，还符合物理规律。

)

## 引入特定的平流方案

下面我们将在一维笛卡尔网格上使用简单的一阶迎风方案，其中单元格 $i$ 的标记密度为 $d_i$，速度为 $u_i$。为简洁起见，我们省略了 $(t)$ 表示时间 $t$ 的量，即，下面的 $d_i$ 写作 $d_i(t)$。从上面，我们将使用我们的物理模型 $P$ 来更新标记密度 $d_i$，这给出以下内容：

$$
d_i(t + \Delta t) = P(d_i(t), u(t), t + \Delta t) = d_i - \Delta t \left[ u^+_i (d_{i+1} - d_i) + u^-_i (d_i - d_{i-1}) \right] \quad \text{其中} \\
u^+_i = \min(u_i / \Delta x, 0) \\
u^-_i = \max(u_i / \Delta x, 0)
$$

> **图 17** 一阶迎风使用一个简单的单侧有限差分模板，考虑流动的方向

因此，对于负的 $u_i$，我们使用 $u^+_i$ 来关注速度的相反方向，即，相对于运动的后向。在这种情况下 $u^-_i$ 将为零。对于正的 $u_i$ 则相反，我们将得到一个为零的 $u^+_i$，并通过 $u^-_i$ 得到一个后向差分模板。以前一种情况为例，对于一个负的 $u_i$，我们有：

$$
P(d_i(t), u(t), t + \Delta t) = (1 + \frac{u_i \Delta t}{\Delta x}) d_i - \frac{u_i \Delta t}{\Delta x} d_{i+1} \tag{17}
$$

因此 $\partial P / \partial u_i$ 得到 $\frac{\Delta t}{\Delta x} d_i - \frac{\Delta t}{\Delta x} d_{i+1}$。直观地说，速度 $u_i$ 的变化取决于密度的空间导数。由于一阶迎风，我们只包括两个邻居（高阶方法将取决于 $d$ 的更多条目）。

在实践中，此步骤等效于评估转置矩阵乘法。如果我们将上面的计算重写为 $P(d_i(t), u(t), t + \Delta t) = A u$，那么 $(\partial P / \partial u)^T = A^T$。

然而，在许多实际情况下，实现此乘法的**无矩阵**（matrix-free）实现可能比实际构造 $A$ 更可取。

我们可以为平流方案考虑的另一个导数是关于先前密度状态的导数，即 $\partial P / \partial d_i(t)$，在缩短的表示法中就是 $\partial P / \partial d_i$。对于上面的单元格 $i$，得到 $1 + \frac{u_i \Delta t}{\Delta x}$。然而，对于完整的梯度，我们需要根据它们速度的符号，添加单元格 $i+1$ 和 $i-1$ 的潜在贡献。这个导数将在下一节中发挥作用。

(

- 一阶迎风格式是 **最简单、稳定** 的平流离散方法：流向哪边，就用哪边的值。
- 它可以写成矩阵乘法，便于理论分析，但在实现中通常用「矩阵-向量乘积」来避免存储大矩阵。
- 对于可微分模拟来说，关键是能计算出关于 **速度场 $u$** 和 **前一状态 $d$** 的梯度，这样才能嵌入到深度学习或反向传播流程中。

)

## 时间演化

到目前为止，我们只处理了从时间 $t$ 到 $t + \Delta t$ 的单个更新步骤，但我们当然可以有任意数量的此类步骤。毕竟，上面我们陈述了将初始标记状态 $d(t_0)$ 推进到时间 $t_e$ 的目标状态 $d_{\text{target}}$ 的目标，这可能涵盖很长的时间间隔。

在上面关于 $\Delta u$ 的表达式中，每个 $d_i(t)$ 又取决于时间 $t - \Delta t$ 的速度和密度状态，即 $d_i(t - \Delta t)$。因此，我们必须追溯我们的损失 $L$ 的影响，一直追溯到 $u$ 如何影响初始标记状态 $d_0$。这可能涉及大量通过 $P$ 对我们的平流方案的评估。

这起初听起来很有挑战性：例如，可以尝试将时间 $t - \Delta t$ 的方程 (17) 插入时间 $t$ 的方程 (17)，并递归地重复此过程，直到我们有一个将 $d_0$ 与目标 $d_{\text{target}}$ 关联起来的单一表达式。然而，由于雅可比矩阵的线性性质，我们将每个平流步骤，即我们的 PDE 的每次调用，视为一个单独的、模块化的操作。

并且这些调用中的每一个都遵循前面部分描述的过程。

给定上面的机制，反向跟踪相当容易实现：对于 $P$ 中的每个平流步骤，我们计算一个雅可比矩阵与来自损失或先前平流步骤的导数向量的乘积。我们重复这个过程，直到我们将来自 $L$ 和 $d_{\text{target}}$ 的链一直追溯到 $d_0$。理论上，速度 $u$ 可以是时间的函数，如 $u(t)$，在这种情况下，我们将为每个时间步 $t$ 获得一个梯度 $\Delta u(t)$。然而，为了简化下面的内容，我们假设我们有一个在时间上恒定的场 $u$，即，我们通过对每个平流步重用相同的速度 $P$。现在，每个时间步将为我们提供一个对 $\Delta u$ 的贡献，我们为所有步骤累积这些贡献。

$$
\Delta u = \frac{\partial d(t_e)}{\partial u} \frac{\partial L}{\partial d(t_e)} +
\frac{\partial d(t_e - \Delta t)}{\partial u} \frac{\partial d(t_e)}{\partial d(t_e - \Delta t)} \frac{\partial L}{\partial d(t_e)} + \cdots \\
\cdots + \left( \frac{\partial d(t_0)}{\partial u} \cdots \frac{\partial d(t_e - \Delta t)}{\partial d(t_e - 2\Delta t)} \frac{\partial d(t_e)}{\partial d(t_e - \Delta t)} \frac{\partial L}{\partial d(t_e)} \right)
$$

这里上面的最后一项包含了标记密度回溯到时间 $t_0$ 的完整路径。这些求和的项起初看起来令人生畏，但仔细看，每一行只是在左边增加了一个额外时间步的雅可比矩阵。这是由链式法则得出的，如上面两个算子的情况所示。因此，求和的项包含许多相似的雅可比矩阵，并且在实践中可以通过向前评估我们的 PDE 所产生的计算步骤序列进行**反向跟踪**（backtracing）来高效计算。（请注意，如上所述，我们在此处省略了雅可比矩阵的转置。）

这种结构也清楚地表明，该过程与神经网络常规训练过程非常相似：通过这些嵌套函数调用评估雅可比向量积正是深度学习框架为训练神经网络所做的（在那里我们有权重 $\theta$ 而不是速度场 $u$）。因此，我们在实践中需要做的所有事情就是为 $P$ 提供一个计算雅可比向量积的自定义函数。

(虽然公式看上去很复杂，但在实现中其实很自然：

1. **正向计算（forward）**：先把 PDE 一步步往前推，得到最终状态 $d(t_e)$。
2. **反向传播（backward）**：从损失函数开始，把梯度一步步传回去，每一步只需要做一次 **雅可比矩阵 × 向量** 的运算，而不是显式地存整个矩阵。
3. 如果速度场 $u$ 是时间不变的（所有时间步共用），那每个时间步都会给 $\Delta u$ 提供一份贡献，最后把它们加起来就行。

)

## 隐式梯度计算

作为一个稍微复杂一点的例子，让我们考虑泊松方程 $\nabla^2 a = b$，其中 $a$ 是感兴趣的物理量，$b$ 是给定的。这是一个非常基本的椭圆型 PDE，对从静电学到引力场的各种物理问题都很重要。它也出现在流体的背景下，其中 $a$ 扮演流体中标量压力场的角色，右边由流体速度 $u$ 的散度 $\nabla \cdot u$ 给出。

对于流体，我们通常有 $u_n = u - \nabla p$，其中 $\nabla^2 p = \nabla \cdot u$。这里，$u_n$ 表示新的无散度速度场。这一步通常对于强制执行硬约束 $\nabla \cdot u_n = 0$ 至关重要，也称为 Chorin 投影或亥姆霍兹分解。它是向量微积分基本定理的直接结果。

如果我们现在引入一个在求解器中修改 $u$ 的神经网络，我们不可避免地要通过泊松求解进行反向传播。即，我们需要一个关于 $u$ 的梯度 $\partial u_n / \partial u$，在这种表示法中，其形式为 $u_n = u - \nabla ((\nabla^2)^{-1} \nabla \cdot u)$。

结合起来，我们的目标是计算 $\partial u_n / \partial u$。外梯度（来自 $\nabla p$）和内散度（$\nabla \cdot u$）都是线性算子，它们的梯度很容易计算。主要困难在于从泊松方程中获取矩阵逆 $(\nabla^2)^{-1}$（我们在这里保持简单些，但它通常是时间依赖且非线性的）。

在实践中，$(\nabla^2)^{-1} b$ 与 $b = \nabla \cdot u$ 的矩阵向量积不是通过矩阵运算显式计算的，而是通过一个（可能是无矩阵的）迭代求解器来近似。例如，共轭梯度（CG）方法在这里是非常受欢迎的选择。因此，理论上我们可以将这个迭代求解器视为一个函数 $S$，其中 $p = S(\nabla \cdot u)$。值得注意的是，矩阵求逆是一个非线性过程，尽管矩阵本身是线性的。由于像 CG 这样的求解器也基于矩阵和向量运算，我们可以将 $S$ 分解为所有求解器迭代过程中一系列更简单的操作 $S(x) = S_n(S_{n-1}(... S_1(x)))$，并通过它们中的每一个进行反向传播。这当然是可能的，但不是一个好主意：它可能引入数值问题，并且会非常慢。如上所述，默认情况下，深度学习框架会为每个可微分算子（如本例中的 $S_i()$）存储内部状态，因此我们将在内存中组织并保存可能大量数量的中间状态。然而，这些状态对于我们原来的 PDE 来说完全无关紧要。它们只是 CG 求解器的中间状态。

如果我们退后一步看 $p = (\nabla^2)^{-1} b$，它的梯度就是 $\partial p / \partial b = ((\nabla^2)^{-1})^T$。在这种情况下，$\nabla^2$ 是一个对称矩阵，所以 $((\nabla^2)^{-1})^T = (\nabla^2)^{-1}$。这与我们在上面原始方程中遇到的逆矩阵相同，因此我们重用我们未修改的迭代求解器来计算梯度。我们不需要将其拆开并通过存储中间状态来减慢它的速度。然而，迭代求解器计算 $(\nabla^2)^{-1} b$ 的矩阵向量积。那么在反向传播过程中 $\partial p / \partial b$ 是什么？在优化设置中，我们总是在前向链的末端有我们的损失函数 $L$。反向传播步骤然后将给出输出的梯度，我们假设这里是 $\partial L / \partial p$，它需要传播到前向传递的早期操作。因此，我们只需在反向传递期间调用我们的迭代求解来计算 $\partial p / \partial b = S(\partial L / \partial p)$。并且假设我们为前向传递选择了一个好的求解器 $S$，我们在反向传递中得到完全相同的性能和精度。

如果你对代码示例感兴趣，phiflow 的 [differentiate-pressure 示例](https://github.com/tum-pbs/PhiFlow/blob/master/demos/differentiate_pressure.py) 正是使用这个过程来优化压力投影步骤：一个在右侧受约束的流场，针对左侧的内容进行优化，使得在压力投影步骤之后它与右侧的目标匹配。

这里的主要收获是：**不要盲目地通过前向计算进行反向传播**，而是要考虑前向传递的解析方程中哪些步骤需要计算梯度，这一点很重要。在上述情况下，我们通常可以找到改进的梯度解析表达式，然后对其进行数值近似。

> **隐函数定理与时间 (IFT & Time)**:
> *   **IFT**: 上面的过程本质上产生了一个**隐式导数**（implicit derivative）。我们没有显式地推导所有前向步骤，而是依赖于**隐函数定理**（implicit function theorem）来计算导数。
> *   **Time**: 我们实际上可以将迭代求解器的步骤视为一个虚拟的“时间”，并通过这些步骤进行反向传播。与其他 DP 方法一致，这使得神经网络能够与迭代求解器交互。一个例子是从 [UBH+20] 中学习 CG 求解器的初始猜测。细节和代码可以[在这里找到](https://github.com/tum-pbs/learning-cg)。

(

- **不要盲目反向传播迭代求解器**，那样会慢且占内存。
- **利用隐函数定理 (IFT)**，直接写出梯度的解析形式。
- 在实践中，前向和反向都用同一个求解器（比如 CG），既高效又精确。

)

## 可微分物理方法总结

总结来说，使用可微分物理模拟为我们提供了一个工具，可以将具有选定离散化的物理方程纳入深度学习中。与前一章的残差约束相比，这使得神经网络能够与物理求解器无缝交互。

我们之前会在神经网络训练完成后完全丢弃我们的物理模型和求解器：在**使用 PINN 进行 Burgers 方程优化**的例子中，神经网络直接给我们解，绕过了任何求解器或模型方程。DP 方法与**物理损失项**中的物理信息神经网络（v2）有很大不同，它与**受控离散化（v1）** 有更多共同点。它们本质上是 DP 训练的一个子集或部分应用。

然而，与两种残差方法相比，DP 使得 alongside 一个数值求解器训练神经网络成为可能，因此我们可以在推理时继续使用物理模型（由求解器表示）。这使我们能够超越解决单一的逆向问题，并产生能够相当稳健地泛化到新输入的神经网络。让我们在 DP 的背景下重新审视**使用 PINN 进行 Burgers 方程优化**中的示例问题。

(

#### 1. DP 的核心思想

- **传统做法（PINN 残差约束）**：我们训练神经网络去直接逼近 PDE 的解。神经网络就是“黑盒子”，训练完以后物理模型就不再用上了。
- **DP 方法**：不让神经网络单独去学解，而是让它和 **数值求解器** 一起工作。
   神经网络只是“调控”一部分，比如边界条件、参数或者源项；然后再交给物理求解器去演化系统。

换句话说，DP 方法把 **神经网络和数值模拟器绑定在一起**，网络不再是替代品，而是一个增强器。

------

#### 2. 和 PINN（残差方法）的区别

- **PINN（物理损失）**：
  - 神经网络必须直接学到 PDE 的解。
  - 一旦训练完成，物理方程和求解器都被丢掉。
  - 泛化能力有限（可能只在训练过的范围内有效）。
- **DP（可微分物理）**：
  - 神经网络只是修改或补充某些量（如参数、外力），真正的 PDE 解仍然由数值求解器算出来。
  - 训练时求解器在计算图中，保证梯度能传回去。
  - 推理时依然能用物理求解器 → 稳健性更好，泛化能力更强。

------

#### 3. 好处

1. **更稳定**：求解器保证了解的物理合理性（比如守恒律）。
2. **泛化更强**：网络不是硬逼解，而是通过物理规律演化，适应新输入更容易。
3. **更灵活**：你可以随时换 PDE、换离散化方式，网络只需要学和它交互的部分。

)