# 将可微分物理（DP）集成到神经网络（NN）训练中

## 目录
- 交换顺序
- 循环评估
- NN 与求解器的组合
- 方程形式
- 通过求解器步骤的反向传播
- 替代方案：噪声
- 复杂示例

我们现在将目标定位于将可微分物理（DP）设置集成到神经网络（NN）中。当将 DP 方法用于学习应用时，在 DP 和 NN 构建块的组合方面具有很大的灵活性。由于一些差异很微妙，以下部分将更详细地介绍。我们将特别关注那些重复执行 PDE 和 NN 评估多次的求解器，例如，为了计算物理系统随时间推移的多个状态。在经典数值方法中，这被称为迭代时间步进方法，而在 AI 的背景下，它是一种自回归方法。

> **提示：校正与预测**
> 最适合用 DP 方法解决的问题是非常基础的。不完美的物理模型 $P$ 和改进项 $f$ 的组合在经典上有许多不同的名称：流体动力学和湍流中的闭合问题，材料科学中的均质化或粗粒化，而在气候和天气中它被称为参数化。
> 在下文中，我们将泛泛地将所有这些包含 NN+求解器 $f \circ P$ 的任务称为**校正任务**，以区别于推理时不涉及求解器的纯**预测任务**。

为了回顾一下，这里是之前关于结合 NN 和 DP 算子的图。在图中，这些算子看起来像一个损失项：它们通常没有权重，只提供一个影响 NN 权重优化的梯度：

> **图 18** 前面章节中描述的 DP 方法。一个网络产生一个 PDE 求解器 $P$ 的输入，该求解器在反向传播步骤中为训练提供梯度。
> ![(示意图：NN -> PDE求解器P -> 损失L，梯度反向传播)](https://i.imgur.com/example1.png) *（此处应有图18：NN输出给P，P输出计算损失L，梯度从L传回P再传回NN）*

这种设置可以看作是网络接收关于其输出如何影响 PDE 求解器结果的信息。即，梯度将提供信息，说明如何产生一个能最小化损失的 NN 输出。类似于前面描述的**物理损失项**，这可以意味着，例如，维护守恒定律或通常是随时间推移的基于 PDE 的约束。

（

##### PDE 与神经网络的结合

- 在数值模拟里，我们常常用 PDE（偏微分方程）来描述系统的演化，比如流体、气候、材料等。
- 在深度学习里，神经网络可以拟合函数、预测趋势、或修正模型。
- 把两者结合起来的关键想法是：
   **让 NN 的输出进入 PDE 求解器，PDE 给出物理约束，然后一起反向传播梯度。**

换句话说：

- 神经网络：产生一些输入（比如初始条件、参数修正、外部力）。
- PDE 求解器：基于物理定律演化状态，生成系统的未来状态。
- 损失函数：衡量模拟结果和观测/目标之间的差距。
- 反向传播：梯度通过 PDE 算子传回神经网络，告诉 NN 应该怎么调整参数。

）

## 交换顺序

然而，对于 DP 来说，没有真正的理由局限于这种设置。例如，我们可以想象交换 NN 和 DP 组件的位置，得到以下结构：

> **图 19** 一个 PDE 求解器 $P$ 产生一个输出，该输出由 NN $f$ 处理。
> ![(示意图：输入 -> PDE求解器P -> NN f -> 输出/损失)](https://i.imgur.com/example2.png) *（此处应有图19：某个输入经过P得到状态，再经过NN f得到输出或损失）*

在这种情况下，PDE 求解器本质上代表一个即时数据生成器。这不一定总是有用：这种设置可以被相同输入的预计算所取代，因为 PDE 求解器不受 NN 的影响。因此，没有通过 $P$ 的反向传播，它可以用一个简单的“加载”函数代替。另一方面，在训练时使用输入参数的随机采样来评估 PDE 求解器可以导致对输入数据分布的极好采样。如果我们有关于输入如何变化的现实范围，这可以改进 NN 训练。如果正确实施，求解器还可以减轻存储和加载大量数据的需要，而是在训练时更快地产生它们，例如，直接在 GPU 上。最近的方法在**主动学习**（Active Learning）的背景下探索这个方向。

然而，这个版本没有利用来自可微分求解器的梯度信息，这就是为什么以下变体更有趣。

（

这里讲的其实是 **把 PDE 求解器和神经网络的顺序对调**，也就是：

- 前一种设置：
   **NN → PDE 求解器 → 损失**
   （NN 影响 PDE，梯度通过 PDE 回传）
- 现在这种设置：
   **输入 → PDE 求解器 → NN → 损失**
   （PDE 先生成结果，NN 再加工处理）

你可以把它理解为：

- PDE 就像一个“物理模拟器工厂”，随时给你吐出一堆流体/烟雾/温度分布。
- NN 拿到这些结果后，学习如何识别/压缩/映射到别的东西。
- 这种方式对 **数据生成** 很方便，但训练时 **没有用到 PDE 的梯度信息**，所以在学习“如何改进 PDE 模型”方面作用不大。

）

## 循环评估

一个特别合理的组合是展开模拟器时间步进过程的迭代，并让系统的状态受到 NN 的影响。（一般来说，只要它们的维度兼容，NN 层和 DP 算子的任何组合都不是被禁止的。）

在展开的情况下，我们在前向传递中计算一个（可能非常长的）PDE 求解器步骤序列。在这些求解器步骤之间，一个 NN 修改我们系统的状态，然后该状态被用于计算下一个 PDE 求解器步骤。在反向传播过程中，我们向后遍历所有这些步骤，以评估对损失函数的贡献（它可以在执行链中的一个或多个地方进行评估），并通过 DP 和 NN 算子反向传播梯度信息。这种求解器迭代的展开 essentially 向 NN 提供了关于其“动作”如何影响物理系统状态和最终损失的反馈。以下是这种组合形式的视觉概述：

> **图 20** 时间步进，其中交织着 DP 和 NN 操作以进行求解器迭代。虚线灰色箭头表示损失项的可选中间评估（类似于最后一步 $k$ 的实心灰色箭头），NN 的中间输出用波浪号 $\sim$ 表示。
> ![(示意图：x0 -> P -> x1 -> f(θ) -> ~x1 -> P -> x2 -> f(θ) -> ~x2 -> ... -> P -> xk -> L，梯度反向传播通过所有步骤)](https://i.imgur.com/example3.png) *（此处应有图20：显示多个时间步，每个步包含P求解和f网络修正，以及损失计算和梯度回传）*

由于这个过程的迭代性质，误差一开始会非常小，然后（对于雅可比矩阵中特征值大于 1 的模式）在迭代过程中缓慢地指数增长。因此，在单次评估中（例如，使用更简单的监督训练设置）极难检测到它们。相反，关键是在训练时向 NN 提供关于误差如何随迭代过程演变的反馈。此外，对于这种迭代情况，预计算状态是不可能的，因为迭代依赖于 NN 的状态。自然，NN 状态在训练时间是未知的，并且在训练过程中会发生变化。这是经典的**数据偏移**（data shift）ML 问题。因此，基于 DP 的训练在这些循环设置中至关重要，以便向 NN 提供关于其当前状态如何影响求解器迭代的梯度，并相应地说明应如何更改权重以更好地实现学习目标。

> 具有许多时间步的 DP 设置可能难以训练：梯度需要通过 PDE 求解器评估和 NN 评估的完整链进行反向传播。通常，它们中的每一个都代表一个非线性和复杂的函数。因此，对于较大数量的步骤，梯度消失和爆炸问题会使训练变得困难。一些减轻这种情况的实际考虑将在**使用神经算子减少数值误差**中介绍。

（

**展开时间步迭代**：模拟通常是“状态 → PDE 更新 → 新状态 → PDE 更新 → …”，逐步往前推进。

在这里，每一步的状态在送入 PDE 之前，都会先经过一次 NN 修正。

所以完整链条是：

```
初始状态 x0
  → PDE P → 状态 x1
  → NN fθ → 修正状态 ~x1
  → PDE P → 状态 x2
  → NN fθ → 修正状态 ~x2
  …
  → PDE P → 状态 xk
  → 损失 L
```

**反向传播**：损失 $L$ 的梯度要反向穿过所有步骤，包括每次 PDE 和 NN 的组合。这样，NN 可以学到“我的改动如何在接下来的很多步里影响结果”。

）

## NN 与求解器的组合

一个我们至今忽略的问题是如何将 NN 的输出合并到迭代求解过程中。在上面的图像中，看起来像是 NN 产生了物理系统的完整状态，该状态被用作 $P$ 的输入。这意味着对于步骤 $j$ 的状态 $x(t + j\Delta t)$，NN 产生一个中间状态 $\tilde{x}(t + j\Delta t) = f(x(t + j\Delta t); \theta)$，求解器用它来产生下一步的新状态：$x(t + (j+1)\Delta t) = P(\tilde{x}(t + j\Delta t))$。

虽然这种方法可行，但它不一定是所有情况下的最佳选择。特别是如果 NN 应该只产生当前状态的校正，我们可以重用当前状态的部分内容。这避免了以 $\theta$ 部分的形式分配 NN 的资源来推断已经正确的部分。沿着 U-Net 中的跳跃连接和 ResNet 中的残差的思想，在这些情况下，最好使用一个算子 $\circ$ 来合并 $x$ 和 $\tilde{x}$，即 $x(t + (j+1)\Delta t) = P(x(t + j\Delta t) \circ \tilde{x}(t + j\Delta t))$。在最简单的情况下，我们可以定义 $\circ$ 为加法，在这种情况下 $\tilde{x}$ 代表 $x$ 的加性校正。简而言之，我们评估 $P(x + \tilde{x})$ 来计算下一个状态。这里网络只需要更新 $x$ 中尚未满足学习目标的部分。

通常，我们可以为 $\circ$ 使用任何可微分算子，它可以是乘法或积分方案。与损失函数类似，这个选择是问题依赖的，但加法通常是一个好的起点。

（

##### 校正式融合

- **思路**：让 NN 只输出一个**校正项**（correction），对当前状态进行小的修正，而不是替代整个状态。

- 具体做法是定义一个合并算子 $\circ$，把原状态 $x$ 和修正 $\tilde{x}$ 融合起来，再交给 PDE。
  $$
  x(t+(j+1)\Delta t) = P\big(x(t+j\Delta t) \circ \tilde{x}(t+j\Delta t)\big)
  $$

- **最简单的选择**：加法
  $$
  x \circ \tilde{x} = x + \tilde{x}
  $$
  这时 $\tilde{x}$ 就表示 NN 学到的“加性修正”。

👉 这样，NN 只需要调整还不准确的部分，不会重复劳动。

）

## 方程形式

接下来，我们将形式化前面段落的描述。具体来说，我们将回答这个问题：就雅可比矩阵而言，$\theta$ 的最终更新步骤是什么样的？

给定具有索引 $i$ 的小批量（mini batches），一个损失函数 $L$，我们将使用 $k$ 表示迭代展开的总步数。为了缩短符号，$x_{i,j} = x_i(t + j\Delta t)$ 表示批次 $i$ 在时间步 $j$ 的状态。用这种符号，我们可以将网络权重的梯度写为：

$$
\frac{\partial L}{\partial \theta} = \sum_i \sum_{m=1}^{k} \left[ \frac{\partial L}{\partial x_{i,k}} \left( \prod_{n=k}^{m+1} \frac{\partial x_{i,n}}{\partial x_{i,n-1}} \right) \frac{\partial x_{i,m}}{\partial \tilde{x}_{i,m-1}} \frac{\partial \tilde{x}_{i,m-1}}{\partial \theta} \right]
\tag{18}
$$

这乍一看不太直观，但这个表达式有一个相当简单的结构：对 $i$ 的第一个求和只是累加一个小批次的所有条目。然后我们有一个对 $m$ 的外部求和（括号），它考虑了从 $1$ 到 $k$ 的所有时间步。对于每个 $m$，我们将通过乘以沿途的所有雅可比矩阵（索引为 $n$，在括号中）来追踪从最终状态 $k$ 回到每个 $m$ 的路径。沿途的每一步都由每个时间步关于 $x$ 的雅可比矩阵组成，而 $x$ 又依赖于来自 NN $f$ 的校正 $\tilde{x}$（未写出）。

在神经网络的最后一步，我们“分叉”并根据网络输出 $\tilde{x}$ 及其在第 $m$ 个时间步的权重 $\theta$ 来确定变化。所有针对不同 $m$ 的贡献被相加，给出一个最终的更新 $\Delta \theta$，用于我们训练过程的优化器。

重要的是要记住，对于大的 $m$，递归应用的 $P$ 和 $f$ 的雅可比矩阵强烈影响后期时间步的贡献，因此关键是稳定训练以防止特别是梯度爆炸。这是我们稍后将多次重新讨论的话题。

在实现方面，所有深度学习框架都会重用针对不同 $m$ 重复的重叠部分。这在反向传播评估中自动处理，并且在实践中，求和将从大的 $m$ 到小的 $m$ 进行评估，这样当我们向小的 $m$ 移动时，可以“忘记”后面的步骤。所以反向传播步骤肯定会增加计算成本，但通常与前向传递的顺序相似，前提是我们有合适的算子来计算 $P$ 的导数。

（

公式 (18) 其实就是 **链式法则 + 时间展开** 的数学写法。

直观地说：**“误差从最后一步往回传，每个时间步都会贡献一份梯度，最终累加得到 NN 参数的更新方向。”**

难点：长序列会导致梯度爆炸/消失，所以训练需要特别小心（比如梯度裁剪、改进数值稳定性的方法）。

）

## 通过求解器步骤的反向传播

现在我们所有这些机制都已建立，一个好问题是：“使用可微分物理模拟器进行训练究竟能在多大程度上改进事情？我们不能简单地展开一个监督设置，沿着标准循环训练的路线，而不使用可微分求解器吗？”或者换句话说，通过求解器的多个步骤进行反向传播，我们真正获得了多少收益？

简而言之，非常多！接下来的段落展示了 List 等人 [LCT22] 的一个湍流混合层的评估案例，以说明这种差异。在深入细节之前，值得注意的是，这个比较使用了一个可微分的二阶半隐式流动求解器，带有一组自定义的湍流损失项。所以这不是一个玩具问题，而是展示了可微分性对于一个复杂的、真实世界案例的影响。

这个案例的好处是，我们可以根据既定的湍流案例统计测量来评估它，并以这种方式量化差异。流的能谱通常是这里的起点，但我们将跳过它，并参考原始论文 [LCT22]，而是专注于两个信息量更大的指标。下面的图表显示了雷诺应力（Reynolds stresses）和湍流动能（TKE），两者都是针对流中一个横截面的已解析量。参考解用橙色点显示。

> **图 21** 使用湍流指标进行量化评估：雷诺应力（左）和 TKE（右）。没有可微分求解器的训练（红色曲线）比使用 DP 的训练（绿色）更强烈地偏离真实情况（橙色点）。
> ![(曲线图：显示雷诺应力和TKE，比较参考、无DP训练、有DP训练和基础求解器的结果)](https://i.imgur.com/example4.png) *（此处应有图21：两条曲线图，显示有DP的训练更接近参考值）*

特别是在彩色箭头指示的区域，“展开监督”训练的红色曲线更强烈地偏离参考解。两次测量都是在使用流体求解器结合训练好的 NN 模拟 1024 个时间步后进行的。因此，两种解决方案都相当稳定，并且明显优于求解器的未修改输出（在图中以蓝色显示）。

当定性地比较涡量场的可视化时，差异也非常明显：

> **图 22** 定性的涡量视觉比较。使用可微分物理求解器进行训练（顶部）产生的结构能更好地保留通过直接数值模拟获得的参考解的结构。
> ![(涡量场对比图：参考DNS、有DP训练、无DP训练、基础求解器)](https://i.imgur.com/example5.png) *（此处应有图22：四张涡量云图，显示有DP的结果更接近DNS参考）*

两种版本（有和没有求解器梯度）都强烈受益于展开，在这个比较中展开了 10 步。然而，没有 DP 的监督变体不能在训练时使用关于 NN 影响的长期信息，因此其能力有限。使用可微分求解器训练的版本接收整个 10 个展开步骤过程的反馈，通过这种方式可以推断出校正，从而为最终的、由 NN 驱动的求解器提供改进的精度。

作为一个展望，这个案例也突出了将 NN 纳入求解器的实际优势：我们可以测量一个常规模拟需要多长时间才能达到湍流统计方面的某种精度。对于这个案例，它将需要比带有 NN 的求解器长 14 倍以上的时间 [LCT22]。虽然这只是一个初步的数据点，但很高兴看到，一旦网络被训练好，或多或少可以开箱即用地实现性能上的现实世界改进。

（

##### 训练展开步数与性能

- 在实验中，两种方法都展开了 10 步来训练（所以不是单步监督）。
- 但只有 DP 能利用这些展开的长期梯度。
- 更重要的是：训练好的 NN 可以让求解器跑得更快。
  - 在这个案例里，要达到同样的湍流统计精度，**传统求解器需要比 NN+DP 的版本多花 14 倍时间**。
  - 这说明 NN 校正不仅能提高精度，还能节省计算资源。

##### 通俗总结

- **展开监督**：NN 就像蒙着眼睛试探，只知道“最后和目标差多少”。
- **DP 反向传播**：NN 像是装了 GPS，能追踪“我在第 1 步的动作是如何影响第 10 步结果的”。
- 结果就是：有 DP 的训练收敛更稳、解更接近真实物理，还能让求解器跑得更高效。

）

## 替代方案：噪声

其他工作提出了在训练时用噪声 [SGGP+20] 扰动输入和迭代，有点类似于像 dropout 这样的正则化器。这有助于防止对训练状态的过拟合，并且通过这种方式可以帮助稳定训练迭代求解器。

然而，噪声在性质上是非常不同的。它通常是无方向的，因此不如使用模拟的实际演化进行训练准确。所以噪声可以作为一个容易过拟合的训练设置的良好起点。但是，如果可能，最好通过 DP 方法将实际求解器纳入训练循环中，以便向网络提供关于系统时间演变的反馈。

就目前情况而言，生成建模方法（去噪扩散或流匹配）或为合并噪声提供了更有根据的方法。我们将在**无条件稳定性**中更详细地研究这个主题。

（

**加噪声训练** = 给系统加点“随机抖动”，防止模型太依赖训练数据。

它确实有用，像是正则化或数据增强。

但它 **不如 DP 真正把 PDE 的时间演化梯度引入训练那么精确**。

所以：

- 如果只是怕过拟合，可以加噪声。
- 如果想真正利用物理规律提升长期预测，最好还是用 DP。

）

## 复杂示例

以下部分将提供更复杂案例的代码示例，以展示通过可微分物理训练可以实现什么。

首先，我们将展示一个采用深度学习来表示数值模拟误差的场景，遵循 Um 等人 [UBH+20] 的工作。这是一项非常基础的任务，要求学习到的模型与数值求解器紧密交互。因此，这是一个典型例子，说明将数值求解器带入深度学习循环至关重要的情况。

接下来，我们将展示如何让 NN 解决棘手的逆向问题，即 Navier-Stokes 模拟的长期控制，遵循 Holl 等人 [HKT19] 的工作。这项任务需要长期规划，因此需要两个网络，一个用于预测演化，另一个用于采取行动以达到所需目标。（稍后，在**使用强化学习控制 Burgers 方程**中，我们将把这种方法与另一种使用强化学习的 DL 变体进行比较。）

这两个案例都需要比先前示例多得多的资源，因此您可以预期这些笔记本运行时间更长（并且在处理这些示例时使用检查点是一个好主意）。