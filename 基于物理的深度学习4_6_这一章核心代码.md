# 使用神经算子减少数值误差

## 目录
- 问题表述
- 开始实现
- 模拟设置
- 网络和转移函数
- 训练设置
- 交织模拟与神经网络
- 测试评估
- 后续步骤

在这个例子中，我们将目标定位于在连续 PDE $P^*$ 离散化（即当我们表述为 $P$ 时）中产生的数值误差。这种方法将证明，尽管缺乏闭式描述，离散化误差通常是具有规则和重复结构的函数，因此可以被离散化的神经算子学习。一旦训练完成，神经网络（NN）可以在本地进行评估，以改进 PDE 求解器的解，即减少其数值误差。最终的方法是一种混合方法：它将始终执行（粗略的）PDE 求解，然后在运行时通过 NN 推断的校正来改进它。

几乎所有的数值方法都包含某种形式的迭代过程：显式求解器随时间重复更新，或隐式或稳态求解器在单个更新步骤内进行迭代。第二种情况的例子可以在这里找到，但下面我们将针对第一种情况，即随时间迭代。[在 colab 中运行]

（

##### 问题背景：数值误差

- 连续 PDE $P^*$（物理真实方程）必须经过**离散化**才能在计算机里求解，记作 $P$。
- 离散化不可避免会带来数值误差：比如波动被过度平滑、相位偏移、震荡等。
- 这些误差虽然复杂，但往往是 **有规律、可重复** 的（取决于时间步长、网格大小等）。

------

##### 核心思路

- 既然这些误差有规律，就可以让神经网络来“学”这些误差。
- 方法流程：
  1. **先跑 PDE 求解器**（给出一个粗略解）。
  2. **再用神经网络**在每个时间步上局部修正解，减少误差。

👉 这就像请 NN 当“校对员”：求解器写出草稿，NN 在旁边检查并改掉常见的错。

）

## 问题表述

在减少误差的背景下，拥有一个可微分物理求解器至关重要，这样学习过程才能考虑求解器的更新。这种交互在基于监督学习或 PINN 的训练中是不可能的。即使监督式 NN 的微小推理误差也会随时间累积，并导致数据分布与预计算数据的分布不同。这种分布偏移会导致次优结果，甚至导致求解器爆炸。

为了学习误差函数，我们将考虑同一 PDE $P^*$ 的两种不同离散化：一个参考版本，我们假设它是准确且高保真的，具有离散化版本 $P_r$ 和解 $r \in R$，其中 $R$ 表示 $P_r$ 的解流形。与此并行，我们有一个相同 PDE 的低保真求解器，我们将其称为源版本，因为这将是我们的 NN 稍后要与之交互的求解器。类似地，我们有 $P_s$ 及其解 $s \in S$。训练后，我们将获得一个混合求解器，它将源求解器与训练好的算子 $f$ 结合以获得改进的解，即更接近 $P_r$ 产生的解。

> **图 23** 粗略和参考流形的视觉概述
> ![(示意图：显示高保真参考流形 R 和低保真源流形 S，以及转移算子 T 和校正算子 C)](https://i.imgur.com/example23.png) *（此处应有图23：两个解流形及其关系）*

假设 $P$ 将一个解推进一个时间步 $\Delta t$，并用上标表示连续步骤：$P^n(s(T r_t)) = P_s(P_s(\cdots P_s(T r_t)\cdots))$。模拟的相应状态是 $s_{t+n} = P^n(T r_t)$。这里我们假设存在一个映射算子 $T$，将参考解转移到源流形。例如，这可能是一个简单的下采样操作。特别是对于较长的序列（即较大的 $n$），源状态 $s_{t+n}$ 将偏离相应的参考状态 $r_{t+n}$。这就是我们将在下面用基于 NN 的算子解决的问题。

和之前一样，我们将使用 $L_2$ 范数来量化偏差，即误差函数 $e(s_t, T r_t) = \|s_t - T r_t\|^2$。我们的学习目标是训练一个校正算子 $C(s)$，使得应用了校正的解比原始未修改的（源）解具有更低的误差：$e(P_s(C(T r_t)), T r_{t+1}) < e(P_s(T r_t), T r_{t+1})$。

校正算子 $C(s|\theta)$ 由一个具有权重 $\theta$ 的深度神经网络表示，并接收状态 $s$ 以推断出一个具有相同维度的加性校正场。为了区分原始状态和校正后的状态，我们用附加的波浪号 $\tilde{s}$ 表示后者。现在的整体学习目标变为：

$$
\arg \min_{\theta} ((P_s C)^n (T r_t) - T r_{t+n})^2
$$

为了简化符号，我们在这里省略了对不同样本的求和（之前版本中的 $i$）。上述方程中一个容易忽略的关键点是，校正依赖于修改后的状态，即它是 $\tilde{s}$ 的函数，所以我们有 $C(\tilde{s}|\theta)$。这些状态在训练时实际上是随时间演化的。它们事先并不存在。

**TL;DR:** 我们将训练一个神经算子，以减少模拟器相对于更准确参考的数值误差。将源求解器实现为可微分物理算子至关重要，这样它可以为改进 $C$ 的训练提供梯度。

（

##### 为什么要修正数值误差？

- 我们有一个连续的 PDE $P^*$，这是“真实物理”。
- 但是计算机必须离散化才能算，这就得到一个数值求解器 $P$，它会带来误差。
- 我们通常会有两类求解器：
  - **高保真参考求解器 $P_r$**：算得很准，但可能很慢。
  - **低保真源求解器 $P_s$**：算得快，但误差大。

目标：用 NN **修正 $P_s$ 的解**，让它更接近 $P_r$ 的结果。

）

## 开始实现

以下复制了 Solver-in-the-loop: learning from differentiable physics to interact with iterative pde-solvers [UBH+20] 中的一个实验，更多细节可以在论文附录的 B.1 节中找到。

首先，让我们导入必要的库，最重要的是 phiflow 和 PyTorch，并处理好设备设置，这样我们就可以专注于有趣的部分……

```python
try:
    import google.colab  # 确保我们在 colab 内部
    !pip install --upgrade --quiet phiflow==3.4
    #!pip install --upgrade --quiet git+https://github.com/tum-pbs/PhiFlow@develop

    # 用于 pbdl-dataset:
    !pip install --upgrade --quiet git+https://github.com/tum-pbs/pbdl-dataset
except ImportError:
    print("This notebook is running locally, please make sure the necessary pip packages are installed.")
    pass

# 输出: This notebook is running locally, please make sure the necessary pip packages are installed.
```

趁此机会，我们也设置随机种子——显然，42 是这里的终极选择 🙂

```python
import os, sys, logging, argparse, pickle, glob, random, pylab, time
from tqdm import tqdm
from phi.torch.flow import *

random.seed(42)
np.random.seed(42)
#math.seed(42)  # phiflow 种子（注意，这在某些后端可能会导致错误；必要时激活）
math.set_global_precision(32)  # 单精度

USE_CPU = 0
TORCH.set_default_device("GPU")
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
if USE_CPU > 0:
    device = 'cpu'
device = torch.device(device)
print("Using device: " + str(device))
# 输出: Using device: cuda:0
```

（

这一节代码就是 **搭建实验运行环境**：

1. 安装好依赖（phiflow、数据集工具）。
2. 导入库。
3. 固定随机数，保证结果可复现。
4. 设定运行设备（GPU 优先）。

运行完后，你就有了一个 **可在 GPU 上用 Phiflow+PyTorch 做可微分物理实验的环境**。

）

## 模拟设置

现在我们设置源模拟 $P_s$。注意，我们在这个笔记本中不会处理 $P_r$：来自高保真求解的下采样参考数据已经包含在训练数据集中。它是用精细四倍的空间和时间离散化生成的。下面我们专注于源求解器和 NN 的交互。

下面的 `KarmanFlow` 求解器模拟了一个相对标准的 Navier-Stokes 尾流案例，在一个矩形域中有一个球形障碍物，并使用显式粘性求解来获得不同的雷诺数。这是设置的几何结构：

> **图 24** 尾流案例的域设置（实现中的大小使用了一个额外的因子 100）。
> ![(示意图：显示计算域，尺寸标注，入口在底部，出口在顶部，障碍物在中间)](https://i.imgur.com/example24.png) *（此处应有图24：显示域尺寸和边界条件）*

求解器对 y 速度应用流入边界条件，使用一个预乘的掩码 (`vel_BcMask`)，在模拟步骤期间设置域底部的 y 分量。这个掩码是使用 phiflow 的 `HardGeometryMask` 创建的，它正确地初始化了交错网格分量的空间偏移条目。模拟步骤相当直接：它计算粘性、流入、平流的贡献，最后通过隐式压力求解使产生的运动不可压缩：

注意，这里的标记密度 $s$ 表示一个被动平流的标记场，而不是流体的密度。下面我们将只关注速度来进行校正任务。标记密度纯粹是为了可视化目的而跟踪的。

```python
RE_FAC_SOL = 10/(128*128)  # 补偿原始缩放的因子

class KarmanFlow():
    def __init__(self, domain):
        self.domain = domain
        self.vel_BcMask = self.domain.staggered_grid(HardGeometryMask(Box(y=(None, 5), x=(None, None))))  # 底部边界掩码
        self.inflow = self.domain.scalar_grid(Box(y=(5,10), x=(25,75)))  # 缩放流入区域
        self.obstacles = [Obstacle(Sphere(center=tensor([50, 50], channel(vector="y,x")), volume=10))]  # 障碍物

    def step(self, marker_in, velocity_in, Re, res, buoyancy_factor=0, dt=1.0):
        velocity = velocity_in
        marker = marker_in
        Re_phiflow = Re / RE_FAC_SOL  # 为 phiflow 重新缩放

        # 粘性
        velocity = phi.flow.diffuse.explicit(u=velocity, diffusivity=1.0/Re_phiflow, dt=dt, substeps=1)

        # 流入边界条件
        velocity = velocity * (1.0 - self.vel_BcMask) + self.vel_BcMask * (1, 0)  # 在底部设置 y 速度

        # 平流
        marker = advect.semi_lagrangian(marker + 1. * self.inflow, velocity, dt=dt)
        velocity = advected_velocity = advect.semi_lagrangian(velocity, velocity, dt=dt)

        # 质量守恒（压力求解）
        pressure = None
        velocity, pressure = fluid.make_incompressible(velocity, self.obstacles, Solve('CG', 1e-5, 1e-5, max_iterations=500, x0=pressure))
        self.solve_info = {'pressure': pressure, 'advected_velocity': advected_velocity}
        return [marker, velocity]
```

（

好的，这里代码是在定义一个**低保真源求解器 $P_s$**，用于模拟流体在障碍物后面的尾流（Kármán vortex street）。我来用通俗的语言解释一下每个部分：

- **高保真求解器 $P_r$** 已经算好了，并且结果被存储在训练数据集里。
- 在这里我们只需要实现一个相对粗糙、低保真的 **源求解器 $P_s$**，然后让神经网络来帮它“修正”误差。
- 这个例子选择了一个经典的流体力学场景：**矩形域里有一个球形障碍物，流体从下往上流动，会在障碍物后面产生涡街**。

这个 `KarmanFlow` 类就是一个**小型流体模拟器**，模拟了流体经过障碍物后形成涡街的过程。
 它的时间推进流程是：

➡ 粘性扩散 → 加入入口速度 → 平流搬运 → 压力投影（不可压）

最终输出的速度场会包含障碍物后面的涡旋。

而在后续实验里，**神经网络会负责修正这个低保真模拟的误差**，让它更接近高保真解。

）

## 网络和转移函数

我们还将定义一个简单的神经网络来表示算子 $C$。我们将使用全卷积网络，即没有任何全连接（MLP）层的网络。我们将使用 phiflow 的网络工具来建立一个 `conv_net`，其层数由 `layers` 列表指定。网络的输入是 3 个场：
*   2 个具有 x, y 速度的场
*   雷诺数作为常数通道。

输出是：
*   一个包含 x, y 速度的 2 分量场。

在卷积网络中，输入维度由输入张量确定（它有三个通道：u, v 和 Re）。然后我们通过一系列卷积层和激活函数处理数据，每层有 32（和 48）个特征，最后在输出中减少到 2 个通道。下面的代码还使用均匀 Xavier 初始化器重新初始化了卷积，该初始化器通过 0.1 的增益进行下调。这通过避免开始时过大的值来简化训练。有了它，我们可以直接激活展开多个步骤。没有它，我们需要添加课程学习（curriculum learning），并确保网络训练一段时间，为校正找到一个合适的范围，然后通过展开应用于更长的序列。

趁此机会，我们（当然）也要检查网络中的参数数量。这是 NN 近似计算成本的关键指标。

```python
layers = [32, 32, 32]  # 小型网络
#layers = [32,48,48,48,32]  # 取消注释以获得一个稍大更深的网络
#network = conv_net(in_channels=3, out_channels=2, layers=layers)  # 一个更简单的变体
network = res_net(in_channels=3, out_channels=2, layers=layers)  # 使用残差网络
print(network)

# 重新初始化
import torch.nn as nn
for m in network.modules():
    if isinstance(m, nn.Conv2d):
        nn.init.xavier_uniform_(m.weight, gain=0.1)

print("Total number of trainable parameters: " + str(sum(p.numel() for p in network.parameters() if p.requires_grad)))
# 输出:
# ResNet( ... ) (网络结构打印)
# Total number of trainable parameters: 47330
```

接下来，我们定义非常重要的转移函数：它们不修改任何内容，而是将模拟状态转换为适合不同软件包使用的数据结构：用于求解器的交错网格，用于 NN 的 PyTorch 张量，以及另一个助手，将数据加载器的 numpy 输出转换为可以轻松在 NS 求解器中使用的 phiflow 网格和值。

`to_phiflow` 函数接受一个 PyTorch 张量批次，并将两个相关通道（第二个维度中索引 1 和 2 对应 x 和 y）转换为 phiflow 的交错网格。这里唯一的注意事项是所涉及数组的大小：由于封闭和开放边界以及交错网格，phiflow 期望 x 和 y 的数组大小分别为 [64, 31] 和 [65, 32]。在 phiflow 的 `math.tensor` 调用中，我们必须告诉 phiflow 关于批处理和空间维度的信息。

`to_pytorch` 函数接收一个包含 `[marker, velocity]` 的小列表，并使用交错网格速度的两个向量分量 `vector['x']` 和 `vector['y']` 来丢弃速度场网格的最外层。这给出了两个大小相等的张量，它们沿着通道维度堆叠。它还通过 `math.ones` 添加了一个常数通道，该通道乘以所需的雷诺数 `ext_const_channel` 到堆栈上。产生的网格堆栈通过 `native(order='batch,channels,y,x')` 转换为单个 PyTorch 张量，并代表神经网络的输入。

```python
def to_phiflow(batch):
    vx = batch[:, 1, :-1, :-1]  # x 速度分量，调整尺寸
    vy = batch[:, 2, :, :]      # y 速度分量，尺寸正确
    # 打印调试信息（可选）
    vel = domain.staggered_grid(math.stack([
        math.tensor(vy, math.batch('batch'), math.spatial('y, x')),  # 注意顺序：y,x 对应 vector 的 y,x
        math.tensor(vx, math.batch('batch'), math.spatial('y, x'))
    ], math.dual(vector="y,x")))
    return vel

def to_pytorch(marker_vel, Re):
    # 对齐交错速度网格的边，使其大小与中心网格相同
    grid = math.stack([
        math.pad(marker_vel[1].vector['x'].values, {'x': (0, 1)}, math.extrapolation.ZERO),  # x 分量，填充
        marker_vel[1].vector['y'].y[:-1].values,  # y 分量，调整尺寸
        math.ones(marker_vel[0].shape) * Re        # 常数雷诺数通道
    ], math.channel('channels')).native(order='batch,channels,y,x')
    return grid

def to_solver(inputs):
    marker_in = inputs[:, 0, :-1, :]  # 标记输入，调整尺寸
    marker_in = domain.scalar_grid(math.tensor(marker_in, math.batch('batch'), math.spatial('y, x')))
    v_in = to_phiflow(inputs)  # 速度输入
    Re = math.tensor(inputs[0, 3, 0, 0].detach())  # 标量雷诺数，获取第一个索引 0,0
    Re_norm = (Re - math.tensor(DATA_RE_MEAN)) / math.tensor(DATA_RE_STD)  # 标准化雷诺数
    Re_norm = float(Re_norm.native().detach())  # 我们只需要一个数字
    return marker_in, v_in, Re, Re_norm
```

最后一个函数是一个助手，用于将数据加载器的输出转换为 phiflow 的数据结构。数据加载器产生 numpy 数组，它们使用上面的 `to_phiflow` 函数转换为速度网格。同时，我们为 NN 算子标准化雷诺数，并为 phiflow 求解器保留原始标量值，因为后者使用“物理单位”，与网络不同。每个训练批次将包含多个具有相同雷诺数的样本。这由数据加载器处理。

（

##### 通俗总结

- 我们建了一个小型卷积残差网络，用来当 **“误差修正器”**。：
- 方式：network = res_net(in_channels=3, out_channels=2, layers=layers)
- 网络输入是 $(u,v,Re)$，输出是修正后的 $(u,v)$。
- 因为 PDE 求解器（Phiflow）和 NN（PyTorch）的数据格式不同，所以写了几个小助手函数：
  - `to_phiflow`：把 NN 张量 → PDE 网格。
  - `to_pytorch`：把 PDE 网格 → NN 张量。
  - `to_solver`：数据加载器输出 → 求解器能用的格式。

这让 PDE 和 NN **能在一个循环里互相交互**，实现“混合模拟+学习”。

其中 def to_solver(inputs):

输入：数据加载器（通常是 numpy 数组 → torch 张量）。

输出：

- **marker_in**：标记密度场（中心网格）。
- **v_in**：速度场（交错网格）。
- **Re**：原始雷诺数（物理单位）。
- **Re_norm**：标准化后的 Re（给 NN 用）。

）

## 训练设置

现在我们也需要设置训练，即初始化数据集和优化器。后者相对容易，我们将使用具有给定学习率的 Adam 优化器，这也是定义批处理大小（这里 3 是一个好的默认值）的好时机。

最重要和最有趣的参数是 `MSTEPS`。它定义了每次训练迭代时展开的模拟步骤数。这直接影响每个训练步骤的运行时间，因为我们首先必须向前模拟所有步骤，然后通过所有 `MSTEPS` 模拟步骤（与 NN 评估交织在一起）反向传播梯度。然而，这正是我们将收到重要梯度反馈的地方，关于推断的校正如何实际影响正在运行的模拟。因此，通常更大的 `msteps` 更好。理想情况下，训练也以课程学习的形式随时间增加 `MSTEPS` 的数量，但为了保持代码更简单，我们将省略这一点。

对于训练数据本身，我们可以使用 PBDL 的数据加载器类，它自动从 HuggingFace 下载数据（如果需要），并返回一个我们可以轻松迭代以获取新训练批次的类。这里我们需要指定 `BATCH_SIZE`，并且我们从数据集中选择模拟 0 到 5，以便将案例 6 到 9 留待以后测试。前 6 个包含一个中间范围的雷诺数，这样我们可以在以后保留一些超出此范围的新雷诺数用于测试泛化。数据加载器还需要提供足够的地面实况参考步骤来计算整个展开轨迹的损失。这是通过 `time_steps=MSTEPS+1, intermediate_time_steps=True` 来确保的。

```python
LEARNING_RATE = 1e-3
optimizer = adam(network, LEARNING_RATE)

# 最关键的参数之一：向前看多少模拟步骤以进行训练
MSTEPS = 4
BATCH_SIZE = 3

import pbdl
import pbdl.torch.loader
dataloader = pbdl.torch.loader.Dataloader("solver-in-the-loop-wake-flow", MSTEPS, sel_sims=[0,1,2,3,4,5],
                                         batch_size=BATCH_SIZE, normalize_const="standard",  # 使用标准归一化
                                         time_steps=MSTEPS+1, intermediate_time_steps=True)

# 使用归一化和非归一化值的变通方法：
# 保存雷诺数调节的归一化常数，然后在脚本中手动对 Re 值进行归一化
DATA_RE_MEAN = dataloader.dataset.norm_strat_const.const_mean[0][0]
DATA_RE_STD = dataloader.dataset.norm_strat_const.const_std[0][0]
print([DATA_RE_MEAN, DATA_RE_STD])
dataloader.dataset.norm_strat_const = None  # 禁用加载器的归一化
dataloader.dataset.norm_strat_data = None
# 输出: [np.float64(1237.79296875), np.float64(1453.7359614526729)] (示例值)
```

此外，我们在下一个代码单元中定义了几个全局变量来控制训练和模拟。

流体求解器对象在下面称为 `simulator`。为了轻松地在 phiflow 中创建网格，它使用了一个 phiflow `Domain` 对象，这主要是为了方便：它存储域的分辨率、物理大小和边界条件。这些信息需要传递给每个网格，因此以 `Domain` 的形式将其放在一个地方很方便。对于上述设置，我们需要沿 x 和 y 的不同边界条件：分别是封闭壁和自由流入流出域。

```python
# 这是 phiflow 的单元实际分辨率（不太关键）
SOURCE_RES = [64, 32]
# 这是域边界框的物理大小（抽象单位）
LENGTH = 100.
# 为了可读性
from phi.physics._boundaries import Domain, OPEN, STICKY as CLOSED
BNDS = {
    'y': (OPEN, OPEN),   # y 方向开放边界
    'x': (CLOSED, CLOSED) # x 方向封闭（无滑移）边界
}
# 注意：Domain 类在较新版本中可能已弃用，推荐使用字典方式
domain = Domain(y=SOURCE_RES[0], x=SOURCE_RES[1], boundaries=BNDS, bounds=Box(x=LENGTH, y=LENGTH))
simulator = KarmanFlow(domain=domain)
```

（

我们用 **Adam 优化器**来训练 NN，学习率 0.001，每次训练看 3 个样本，模拟展开 4 步。

数据来自 **PBDL 数据集**，其中包含高保真 Navier-Stokes 模拟，用于指导低保真求解器的校正。

PDE 求解器（Phiflow 的 KarmanFlow）在一个 **64×32 的矩形网格**里模拟流体尾流，底部有流入，上下开放，左右封闭。

）

## 交织模拟与神经网络

为了高效地运行具有非平凡模拟的训练，将运行时记在心中是一个好主意。对于高效运行，特别重要的是涉及用于训练的 GPU，并尽可能将数据保留在 GPU 上。对于 phiflow，这可以通过 jit 编译核心步骤来实现，在下一个代码单元中，我们将为 Navier-Stokes 模拟步骤这样做。它涉及一个隐式压力求解等，并且可能为每个前向和后向传递多次调用。因此，这是一个很好的 jit 编译候选者。（尝试移除 `@jit_compile` 语句以亲身体验速度减慢。）

接下来是整个设置中最关键的步骤：我们定义一个函数，封装每个训练步骤中模拟步骤和网络评估的链。借助我们目前设置的辅助函数，它实际上非常简单：我们循环 `MSTEPS` 次，通过 `simulation_step` 为输入状态调用模拟器，然后通过 `network(to_pytorch(...))` 评估校正算子。然后将 NN 校正添加到预测状态列表中的最后一个模拟状态。这个列表为每个时间步保留了标记密度和速度。

请注意，除了雷诺数，我们没有对状态本身进行归一化，因为它们已经在 -1 到 1 的范围内。对于其他模拟，在调用网络之前进行归一化，并在随后的物理求解步骤之后进行反归一化是一个好主意。

```python
@jit_compile  # 使用 phiflow 的 JIT 编译加速模拟步骤
def simulation_step(marker, velocity, Re, resolution):
    m, v = simulator.step(
        marker_in=marker,
        velocity_in=velocity,
        Re=Re,
        res=resolution,
        dt=1.0  # 假设时间步长
    )
    return m, v

def training_step(inputs_targets):
    [inputs, targets] = inputs_targets
    marker_in, v_in, Re, Re_norm = to_solver(inputs)  # 转换到求解器格式

    prediction = [[marker_in, v_in]]  # 存储预测状态序列
    loss = 0

    for i in range(MSTEPS):
        # 1. 使用模拟器步进
        m2, v2 = simulation_step(
            marker=prediction[-1][0],
            velocity=prediction[-1][1],
            Re=Re,
            resolution=SOURCE_RES[1]
        )

        # 2. 准备网络输入 (当前模拟状态 + 归一化的 Re)
        net_in = to_pytorch([m2, v2], Re_norm)
        # 3. 通过网络获取校正
        net_out = network(net_in)

        # 4. 处理网络输出，调整尺寸以匹配交错网格
        cy = net_out[:, 1, :, :]  # y 分量校正
        cy = torch.nn.functional.pad(input=cy, pad=(0, 0, 0, 1), mode='constant', value=0)  # 填充 y
        cx = net_out[:, 0, :, :-1]  # x 分量校正，调整尺寸

        # 5. 将校正张量转换为 phiflow 交错网格
        v_corr = domain.staggered_grid(math.stack([
            math.tensor(cy, math.batch('batch'), math.spatial('y, x')),  # y 校正分量
            math.tensor(cx, math.batch('batch'), math.spatial('y, x'))   # x 校正分量
        ], math.dual(vector="y,x")))

        # 6. 应用校正：新状态 = 模拟状态 + 网络校正
        new_marker = domain.scalar_grid(m2)  # 标记直接使用模拟结果（可选：也可校正）
        new_velocity = v2 + v_corr           # 速度应用校正

        prediction.append([new_marker, new_velocity])

        # 7. 计算损失：校正后状态与目标状态的差异 (L2 loss)
        vdiff = new_velocity - to_phiflow(targets[:, i, ...])  # 目标需要对应第 i 步
        loss += field.l2_loss(vdiff)

    return loss, prediction
```

上面的 `training_step` 函数也直接评估并返回损失。这里，我们简单地使用整个序列（即展开的 `msteps`）上网格（phiflow 场）的 $L_2$ 损失。在 `vdiff` 中，我们简单地计算目标与当前预测之间的差异，然后计算其 `l2_loss`。

有了训练步骤，训练就相当简单了：剩下要做的就是让优化器计算梯度以最小化损失。Phiflow 为此提供了一个辅助函数：`update_weights`。我们提供神经网络、优化器和计算损失的函数（`training_step` 的第一个返回值）。我们简单地循环 `EPOCHS` 次，枚举数据加载器中的完整数据集。下面的进度条 `pbar` 只是为了跟踪训练进度。由于模拟器的 jit 编译在第一步触发需要更长一点时间，但后续的应该会快得多。下面的代码还在每个 epoch N 将网络状态保存在文件 `net-N.pickle` 中。

```python
EPOCHS = 5
pbar = tqdm(initial=0, total=EPOCHS * len(dataloader), ncols=96)

for epoch in range(EPOCHS):
    for b, (input_cpu, targets_cpu) in enumerate(dataloader):
        # 将数据移动到设备 (GPU)
        input = torch.tensor(input_cpu, dtype=torch.float32).to(device)
        targets = torch.tensor(targets_cpu, dtype=torch.float32).to(device)

        # 执行训练步骤（前向，计算损失，反向传播，更新权重）
        loss, prediction = update_weights(network, optimizer, training_step, [input, targets])

        # 更新进度条描述
        pbar.set_description("loss " + str(np.sum(loss.numpy("batch"))), refresh=False)
        pbar.update(1)

    # 每个 epoch 后保存网络状态
    torch.save(network.state_dict(), "net-" + str(epoch) + ".pickle")

pbar.close()
# 输出: loss ... (训练过程中的损失值输出)
```

请注意，每次对 `update_weights` 的调用在内部执行所有 4 个交织的模拟步骤和神经网络调用，并为整个链反向传播梯度。由于顺序（先求解器，后网络），第一步不会通过求解器反向传播。然而，对于接下来的 3 步，第一次网络调用将收到反馈，说明其推断的校正是否将后续 3 个状态朝着正确的方向影响。

使用默认参数，损失应该从最初的 10 以上下降到 1.0 左右（更多的周期和更大/更深的网络会使其进一步下降）。下降的损失是一个好迹象，但当然，更重要的是要看到产生的 NN-求解器组合在新的输入上表现如何。通过这种训练方法，我们已经实现了一个混合求解器，由一个常规的源模拟器和一个网络算子组成，该网络算子被训练来专门与这个模拟器交互，用于选定的模拟案例领域。

（

## 核心思路

1. **模拟器 (PDE Solver)**：负责每一步 Navier-Stokes 或 Burgers 等物理演化。
2. **神经网络 (NN)**：在每个时间步插入，对物理解算结果做“误差修正”。
3. **交织运行**：每次训练时，先让 PDE 跑一步，再让 NN 修正，然后进入下一步 → 形成一个“展开的链条”。
4. **梯度回传**：最终损失 = 预测的物理状态 vs 高保真参考数据，误差反向通过 NN 和 PDE 求解器 → 更新 NN 权重。

### **训练步骤函数**

```python
def training_step(inputs_targets):
    [inputs, targets] = inputs_targets
    marker_in, v_in, Re, Re_norm = to_solver(inputs)  # 转换成 PDE 格式

    prediction = [[marker_in, v_in]]  # 存储状态
    loss = 0

    for i in range(MSTEPS):
        # 1. PDE 前进一步
        m2, v2 = simulation_step(prediction[-1][0], prediction[-1][1], Re, SOURCE_RES[1])

        # 2. 准备 NN 输入
        net_in = to_pytorch([m2, v2], Re_norm)
        net_out = network(net_in)  # NN 修正输出

        # 3. 转换 NN 输出 → PDE 可用的校正量
        cy = net_out[:, 1, :, :]  # y 修正
        cy = torch.nn.functional.pad(cy, (0, 0, 0, 1), mode='constant', value=0)
        cx = net_out[:, 0, :, :-1]  # x 修正

        v_corr = domain.staggered_grid(math.stack([
            math.tensor(cy, math.batch('batch'), math.spatial('y, x')),
            math.tensor(cx, math.batch('batch'), math.spatial('y, x'))
        ], math.dual(vector="y,x")))

        # 4. 应用校正
        new_marker = domain.scalar_grid(m2)    # 标记场直接用 PDE
        new_velocity = v2 + v_corr             # 速度加上 NN 修正
        prediction.append([new_marker, new_velocity])

        # 5. 损失函数（L2差）
        vdiff = new_velocity - to_phiflow(targets[:, i, ...])
        loss += field.l2_loss(vdiff)

    return loss, prediction
```

👉 这个函数干了三件事：

- 运行 **MSTEPS 步 PDE**，每步都插入一次 NN 修正；
- 每一步都计算 NN 校正后的速度；
- 用 **L2 Loss** 衡量预测 vs 目标差距。

）

## 测试评估

为了评估我们的“AI 驱动求解器”的性能，我们本质上只需要为更多步骤重复每个训练迭代的内部循环。虽然我们在训练时仅限于 `msteps` 评估，但我们现在可以运行我们的求解器任意长度。这是一个很好的测试，可以检验我们的求解器在多大程度上学会了将数据保持在期望的分布内，并代表了更长 rollout 的泛化测试。

我们还需要一组新数据。下面，我们将使用上面下载的数据集中之前未使用的最后四个模拟。我们只需用 `sel_sims=[6,7,8,9]` 实例化一个新的数据加载器。

如果你有一个预训练的网络，这是一个加载模型的好时机。默认情况下，我们假设上面的训练已完成，因此不需要加载任何东西。

我们可以重用上面的大部分求解器代码，但在下面，我们将考虑两个模拟版本：为了比较，我们将在源流形中运行一个参考模拟（即基于 $P_s$，不应用任何校正）。第二个版本是实际结果，我们将重复计算源求解器加上学习到的校正。

下面的 `run_sim` 函数根据是否在 `network` 中提供了神经算子在这两种变体之间切换。没有它，它只是运行源求解器并附加状态。有了网络，它运行完整的混合求解器。两种情况都会沿途计算相对于参考的误差。为了后续分析和可视化，该函数返回校正和参考以及相对误差和求解器计算的实际状态。

```python
def run_sim(inputs, targets, steps, network=None):
    marker_in, v_in, Re, Re_norm = to_solver(inputs)
    simtype = "With corr." if network else "Sim. only"
    print("Running test with Re=" + str(Re) + ", " + simtype)

    prediction = [[marker_in, v_in]]  # 预测状态序列
    correction = [[marker_in, v_in]]  # 存储校正（如果适用）
    refs = [v_in]                     # 存储参考状态
    errors = []                       # 存储相对误差

    for i in tqdm(range(steps), desc=simtype, ncols=64):
        # 模拟步骤
        marker_sim, v_sim = simulation_step(
            marker=prediction[-1][0],
            velocity=prediction[-1][1],
            Re=Re,
            resolution=SOURCE_RES[1]
        )

        if network:  # 运行带有训练好的神经算子的混合求解器
            net_in = to_pytorch([marker_sim, v_sim], Re_norm)
            net_out = network(net_in)

            # 处理网络输出以创建校正场
            cy = net_out[:, 1, :, :]
            cy = torch.nn.functional.pad(input=cy, pad=(0, 0, 0, 1), mode='constant', value=0)
            cx = net_out[:, 0, :, :-1]

            v_corr = domain.staggered_grid(math.stack([
                math.tensor(cy, math.batch('batch'), math.spatial('y, x')),
                math.tensor(cx, math.batch('batch'), math.spatial('y, x'))
            ], math.dual(vector="y,x")))

            new_velocity = v_sim + v_corr
            prediction.append([domain.scalar_grid(marker_sim), new_velocity])
            correction.append([domain.scalar_grid(marker_sim), v_corr])  # 存储校正本身

        else:  # 仅运行低保真求解器
            new_velocity = v_sim
            prediction.append([domain.scalar_grid(marker_sim), new_velocity])

        # 获取当前步骤的参考状态
        ref_vel = to_phiflow(targets[:, i, ...])  # 假设 targets 包含第 i 步的参考
        refs.append(ref_vel)

        # 计算并存储相对 L1 误差 (|预测 - 参考| / |参考|)
        vdiff = prediction[-1][1] - ref_vel
        error_abs = field.l1_loss(vdiff)
        error_rel = error_abs / field.l1_loss(ref_vel)
        errors.append(float(error_rel.native("batch")[0]))  # 假设批次大小为1

    return errors, prediction, refs, correction

# 获取一个测试样本
del dataloader  # 关闭可能存在的旧加载器文件句柄
# 获取新的未见过的测试数据
dataloader_test = pbdl.torch.loader.Dataloader("solver-in-the-loop-wake-flow", 200,
                                              sel_sims=[6, 7, 8, 9], normalize=False,
                                              intermediate_time_steps=True)
# 输出: Download completed ... Success: Loaded ... with 10 simulations (4 selected) and 300 steps.

# 可选加载最终训练好的网络
if False:  # 设置为 True 如果需要加载
    fn = "net-" + str(EPOCHS - 1) + ".pickle"
    network.load_state_dict(torch.load(fn, map_location=device))
    print("Loaded " + fn)

# 从测试加载器获取一个样本
(input_cpu, targets_cpu) = next(iter(dataloader_test))
input = torch.tensor(input_cpu, dtype=torch.float32).to(device)
targets = torch.tensor(targets_cpu, dtype=torch.float32).to(device)
print("Re ", math.tensor(input[0, 3, 0, 0].detach()))  # 打印该样本的雷诺数
# 输出: Re 73.24219 (示例值)

ROLLOUT_STEPS = 100  # 测试运行的步数
# 运行纯源求解器
err_lowfid_only, prediction_lowfid_only, refs, _ = run_sim(input, targets, ROLLOUT_STEPS, network=None)
# 运行混合求解器
err_corrected, prediction_corrected, _, corrs = run_sim(input, targets, ROLLOUT_STEPS, network=network)

print("\n Rel. L2 errors: low-fidelity:", float(np.mean(err_lowfid_only)), " corrected:", float(np.mean(err_corrected)))
# 输出示例: Rel. L2 errors: low-fidelity: 0.10863638424314559 corrected: 0.031211425131186844
```

当然，有趣的问题是：NN 算子在多大程度上真正提高了低保真源求解器的精度？这是由 `run_sim` 函数每次计算的相对误差所捕获的。它测量了与参考速度平方幅度相比的平方距离。上面的单元直接绘制了聚合误差。

下面，我们运行带有训练好的神经校正算子的混合求解器和单独的低保真求解器以进行比较。下面的 `ROLLOUT_STEPS` 决定了用两种变体计算的时间步数。该单元还直接输出平均相对误差。

由于随机训练和数值舍入误差，结果在不同的运行中可能略有不同，但通常，带有神经校正算子的混合求解器应该显示出显著降低的数值误差：通常比低保真求解器低 5-6 倍。这表明此训练设置的中心目标已经实现：**混合求解器产生显著减少的数值误差，并且可以泛化到新的雷诺数** 👍

现在我们将更详细地绘制和比较两个版本。下一个图表显示了误差如何随时间演变。低保真求解器的相对误差随时间线性上升，而混合求解器产生的上升要慢得多。对于更大的网络和成功的训练运行，它几乎可以抑制任何增加，并将误差保持在非常低的水平。这表明，它可以成功地微调低保真求解器以保持目标的精度。请注意，如果没有基础求解器，这项工作会更加困难：那么 NN 需要完成所有工作。在这里，它可以依赖耦合求解器的预测，通常一个小的校正就足够了。

```python
fig = pylab.figure().gca()
pltx = np.linspace(0, ROLLOUT_STEPS - 1, ROLLOUT_STEPS)
fig.plot(pltx, err_lowfid_only, lw=2, color='mediumblue', label='Source')
fig.plot(pltx, err_corrected, lw=2, color='green', label='Hybrid')
pylab.xlabel('Time step')
pylab.ylabel('Relative L2 error')
fig.legend()
# (显示图表: X轴时间步，Y轴相对L2误差，两条曲线)
```

虽然量化的结果给出了我们神经算子性能的重要总结，但对这些结果进行完整性检查以确保 NN 按预期工作是很重要的。在下一个单元中，我们将并排绘制参考、低保真求解器和混合求解器的状态。此外，我们将在右侧绘制两个求解器所犯的错误。

```python
# 显示哪个时间步和哪个批次，默认显示第一个案例的最后一步
STEP = ROLLOUT_STEPS - 1  # 最后一步
BATCH = 0  # 第一个批次
NUM_SHOW = 4
PRINT_STATS = False  # 可选，打印统计信息

fig, axes = pylab.subplots(1, 4, figsize=(16, 5))
i = 0

# 1. 参考速度 (y 分量)
v_ref = refs[STEP].staggered_tensor().numpy('batch,y,x,vector')[BATCH, :, :, 0] # 获取 y 分量
if PRINT_STATS:
    print(["reference ", BATCH, i, np.mean(v_ref), np.min(v_ref), np.max(v_ref)])
axes[i].set_title(f"Ref")
im = axes[i].imshow(v_ref, origin='lower', cmap='magma')
pylab.colorbar(im)
i += 1

# 2. 低保真求解器速度 (y 分量)
v_lowfid = prediction_lowfid_only[STEP][1].staggered_tensor().numpy('batch,y,x,vector')[BATCH, :, :, 0]
if PRINT_STATS:
    print(["low-fid. ", BATCH, i, np.mean(v_lowfid), np.min(v_lowfid), np.max(v_lowfid)])
axes[i].set_title(f"Low-fid.")
im = axes[i].imshow(v_lowfid, origin='lower', cmap='magma')
pylab.colorbar(im)
i += 1

# 3. 校正后（混合）速度 (y 分量)
v_corr = prediction_corrected[STEP][1].staggered_tensor().numpy('batch,y,x,vector')[BATCH, :, :, 0]
if PRINT_STATS:
    print(["corrected", BATCH, i, np.mean(v_corr), np.min(v_corr), np.max(v_corr)])
axes[i].set_title(f"Corr.")
im = axes[i].imshow(v_corr, origin='lower', cmap='magma')
pylab.colorbar(im)
i += 1

# 4. 误差对比: 低保真误差 和 校正后误差
err_lf = v_ref - v_lowfid
err_corr_val = v_ref - v_corr
# 将两个误差图像并排放置
v_errors = np.concatenate([err_lf, err_corr_val], axis=1)
axes[i].set_title(f"Errors: Low-fid (L) & Learned (R)")
im = axes[i].imshow(v_errors, origin='lower', cmap='cividis') # 使用适合误差的色谱
pylab.colorbar(im)

pylab.tight_layout()
```

这非常清楚地显示了中间纯源模拟与左侧参考的偏差。学习到的版本更接近参考解。

右侧的两个逐单元误差图像也说明了这一点：源版本有更大的误差（即更亮的颜色），显示它如何系统地低估了应该形成的涡旋。学习版本的误差分布更均匀，幅度明显更小。

我们的评估到此结束。请注意，AI 驱动的混合求解器的改进行为可能难以使用简单的向量范数（如 MAE 或 $L_2$ 范数）可靠地测量。为了改进这一点，我们需要采用其他特定领域的指标。在这种情况下，基于涡量和流动湍流特性的流体指标将是适用的。然而，在本文中，我们更想专注于 DL 相关主题，并在下一章中使用可微分物理求解器瞄准另一个逆向问题。

（

## 测试流程分解

1. **新数据**
   - 训练时只用了前 6 组尾流案例（Re=不同范围），测试时我们换成第 7–10 组（`sel_sims=[6,7,8,9]`）。
   - 这样可以检验网络是不是学到了泛化能力，而不仅仅是记住训练数据。
2. **两种版本对比**
   - **低保真求解器 (Ps)**：直接跑 PDE（粗网格、步长大），不加修正。
   - **混合求解器 (Ps + NN)**：每一步 PDE 解出结果后，让 NN 修正一次。
3. **误差度量**
   - 每一步和参考高保真解对比，计算 **相对误差**（预测值 vs 真值差距 / 真值幅度）。
   - 得到一条误差随时间的曲线。
4. **结果**
   - 低保真 PDE 的误差会随时间线性增长。
   - 加上 NN 校正后，误差增长速度显著降低（甚至保持几乎平稳）。
   - 平均相对误差通常能从 **0.1+ 降到 0.03 左右**（低 5–6 倍）。

）



## 后续步骤

*   关闭可微分物理训练（通过设置 `msteps=1`），并将其与展开版本进行比较。这产生监督训练，因为不再需要梯度流经求解器。相对误差将 substantially larger（ substantially larger）。
*   同样，尝试使用更大的 `msteps` 设置（例如，8 或 16）训练网络。请注意，由于训练的循环性质，您可能必须加载预训练状态以稳定最初的迭代（这有效地添加了“课程学习”）。
*   使用外部 github 代码生成更艰难的测试数据，并在这些案例上运行您训练好的 NN。您会看到降低的训练误差并不总是与改进的测试性能直接相关。